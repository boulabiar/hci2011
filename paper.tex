\documentclass{llncs}
%
\usepackage{graphicx}
\DeclareGraphicsExtensions{.png}

\newcommand\ignore[1]{}
\newcommand{\imagepathext}[5]{%
\begin{figure}[hbtp]
\hfil\includegraphics[#1]{#2/#3.#4}\hfil
\caption{#5\label{#3}}
\end{figure}}

\newcommand{\image}[3]{\imagepathext{#1}{FIGURES}{#2}{eps}{#3}}
\newcommand{\png}[2]{\imagepathext{width=\columnwidth}{pics}{#1}{png}{#2}}

%

\begin{document}

\frontmatter          % for the preliminaries
\pagestyle{headings}  % switches on printing of running heads
\mainmatter              % start of the contributions
%
\title{A Low-cost Natural Interaction based on a Camera Hand-Gestures Recognizer}
\author{Mohamed-Ikbel Boulabiar\inst{1} \and Thomas Burger\inst{2}
Franck Poirier\inst{3} \and Gilles Coppin\inst{1}}
%
\institute{LAB-STICC, Telecom-Bretagne, France,\\
\email{boulabiar@gmail.com, gilles.coppin@telecom-bretagne.eu},
\and
LAB-STICC, University of Bretagne-Sud, France
\email{thomas.burger@univ-ubs.fr}
\and
VALORIA, University of Bretagne-Sud, France
\email{franck.poirier@univ-ubs.fr}
}

\maketitle

\begin{abstract}
The abstract should summarize the contents of the paper
using at least 70 and at most 150 words. It will be set in 9-point
font size and be inset 1.0 cm from the right and left margins.
There will be two blank lines before and after the Abstract. \dots
\keywords{natural gesture interaction, low-cost gesture recognition, interactive TV broadcast}
\end{abstract}
%

\section{Introduction}
HCI research focuses more attention than before on enhancing the user experience regarding human-display interaction in order to interact more naturally without the need of clicking on buttons and touching screens. However, in everyday life interaction with home devices having embedded computational power, such as interactive TVs, one does not make benefit of these new interaction paradigms yet. 

Testing Refrences \cite{Gesturecraft} and \cite{skinColorSeg}.

\section{Gesture recognition}
First definition of gestures come from the interaction between two human in a discussion, these type of gestures is used to add more precision on the details and according to recent paper, is made by the same mental region of the speech []. An extended meaning come also from gestures language for dumb people.
Gesture recognition got the attention of HCI researchers in order to find better ways to talk to the computer. The first one was the Hand Gestural Cursor (Put-That-There[]) aiming to the simplification of the means of communication between human and the machine.
Since then, the gesture definition included more techniques and got applied on the 2D recognition which is the multi-touch gestures.


\section{Gestures taxonomies}
By looking in the possible gestures human can generate, we get a huge amount of possibilities so there is a need for a taxonomy. Many possible taxonomies exists according to the point of view:

* Taxonomy based on the Gesture Creation Space:
We use this criteria to distinguish between gestures made in the 2D or 3D space.

* Taxonomy based on the number of strokes used in the gesture.
We use this criteria to distinguish between gestures constructed by only one path and between those using many path, so the recognition needs to wait for all strokes before the recognition

* Taxonomy based on gesture use:
We use this technique to distinguish gesture based on the specific use of it, gestures for disabled/dumb people, gestures in talk-shows, gestures in meetings, etc...

\section{Gestures in Talk-shows}
We have analyzed some talk-shows in order to extract important gestures being performed.
(Get the Study from the report + pics + refs)

Steps for skin-color based gesture detection

\subsection{Color Manipulations}
The video processing is based on several modules. First, a basic but adaptive skin color segmentation of a large interval of human skin varieties is performed. 
We do an histogram tuning to better enlarge the colors to all the space available.
Then we transform the color space from BGR to YCrCb because it’s the one with the less parameters where we can do easy segmentation. The transformation from these two color spaces is chosen also because it’s linear []

\begin{center}
\begin{tabular}{c|c|c|c|c}
\textbf{Space} & RGB & HSV & YCrCb & CIE Lab\\
  & R>95, G>40, B>20, & H[0.4 .. 0.7] & Cb[77 .. 127] & C[0 .. 65] \\
\textbf{Param.} & Max\{R,G,B\}-Min\{R,G,B\} < 15 & S[0.15 .. 0.75] & Cr[133 .. 173] & I[0 .. 14] \\
  & abs(R-G) > 15, R > G, R>B & V[0.35 .. 0.95] & &
\end{tabular}
\end{center}

\subsection{Input Filtering}
Then, a morphological kernel is applied to smooth the results, this is necessary to clean the image from lonely pixels and holes inside skin zones.
Each blob is labeled in linear time we use the algorithm [], which requires 1 to 4 passes to recognize all blobs with their bounding path.
We identify the hands and the head by a calibration focused on their first position in the screen. We identify them and add a tag to their respective blobs.

\subsection{Blob Detection}
After having the hand and head blobs identified, an efficient tracking method based on the occlusion is used. It re-triggers a phase of calibration when losing the logic of tracking.
A bounding box distant measure similar to that found in the paper \cite{app06} ("distantBlobTrack"). A distance matrix is also used ('Proximity matrix calculation and "used blob" list initialization') like the one explained in the paper. 

When the matrix of distance is set up, there are some loops:
* A loop for detect inactive tracks: those tracks with no blobs near.
* Detect and create new tracks: those blobs without a track near.
* A loop to assign blobs to tracks. It makes clusters with blobs that are close and assign them to a track. In this step some tracks could merge in one.
* At the end, all inactive tracks are checked to delete the old ones.

To better handle blobs, the bounding path of the blobs is simplified to a set of few points when needed, this is used to identify picks for the counter mode.

%Fig. Skin segmentation
% Testing graphics
\png{skincolor}{Skin color}


\section{Simplified gesture generation}
Using these blocks we are able to perturb


\section{Recognition modes}
The output of this video processing is the input of the recognition module, which is based on the blob location on the screen. Four different modes are defined according to the hand locations and the switch between them is based on the grammar:

1. The movement mode:
it is activated when two hands are close to each other, so that the user selects between 4 directions. This mode can be used as input for games to select between the directions.

2. The mouse mode:
it is used to control a cursor to select something in an arbitrary place on the screen. With this mode, we only get the input from a sub zone in the screen and map it to the full screen. We use it in a similar way to a computer touchpad.
The user can validate clicks using a tempo on another zone.

3. The counter mode: counts the number of fingers in a pre-selected hand (the left and right hand are automatically discriminated) by counting the peaks in the simplified contour of the blob. This mode can again be used in games for kids.

4. The multi-touch mode, where we consider the similarities between 2D gestures in a tactile multi-touch context and gestures in the 3D space. To do so, we consider the hands blobs in a manner similar to that of two finger tips in the input of a multi-touch device. Based on that, we can recognize well known multi-touch gestures, such as Drag, Pinch/Zoom and Rotate but applied on our case.


\section{Usability analysis and tests}


% ---- Bibliography ----
\bibliographystyle{splncs03}
\bibliography{paper}

\end{document}
