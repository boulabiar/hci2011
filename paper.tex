\documentclass{llncs}
%
\usepackage{graphicx}
\DeclareGraphicsExtensions{.eps}

\newcommand\ignore[1]{}
\newcommand{\imagepathext}[5]{%
\begin{figure}[!htbp]
\hfil\includegraphics[#1]{#2/#3.#4}\hfil
\caption{#5\label{#3}}
\end{figure}}

\newcommand{\image}[3]{\imagepathext{#1}{pics}{#2}{eps}{#3}}
\newcommand{\png}[2]{\imagepathext{width=\columnwidth}{pics}{#1}{png}{#2}}


%

\begin{document}

\frontmatter          % for the preliminaries
\pagestyle{headings}  % switches on printing of running heads
\mainmatter              % start of the contributions
%
\title{A Low-cost Natural Interaction System based on a Camera Hand-Gestures Recognizer}
\author{Mohamed-Ikbel Boulabiar\inst{1} \and Thomas Burger\inst{2}
Franck Poirier\inst{3} \and Gilles Coppin\inst{1}}
%
\institute{LAB-STICC, Telecom-Bretagne, France,\\
\email{boulabiar@gmail.com, gilles.coppin@telecom-bretagne.eu},
\and
LAB-STICC, University of Bretagne-Sud, France
\email{thomas.burger@univ-ubs.fr}
\and
VALORIA, University of Bretagne-Sud, France
\email{franck.poirier@univ-ubs.fr}
}

\maketitle

\begin{abstract}
The search for new simplified interaction techniques is 
mainly motivated by the improvements of 
the communication with interactive devices.
In this paper, we present an interactive TVs module capable
of recognizing human gestures through the PS3Eye low-cost camera.
We recognize gestures by the tracking of human skin blobs and analyzing the 
corresponding movements. It provides means to control a TV in an ubiquitous
computing environment.
We also present a new free gestures icons library created to allow easy
representation and diagramming.
%after an analysis of TV's Talk-shows.

\keywords{natural gesture interaction, low-cost gesture recognition, interactive
TV broadcast, ubiquitous computing}
\end{abstract}
%

\section{Introduction}
HCI research focuses more attention than before on enhancing the user
experience regarding human-display interaction.
This area of research and specifically gesture, work on making interact natural
without the need of clicking on buttons and touching screens with first systems
since the 1980's \cite{Bolt1980}, \cite{Pavlovic97visinter}, \cite{Derpanisreview}.
However, in everyday life, interaction with home devices having
embedded computational power, such as interactive TVs,
one does not make benefit from these new interaction paradigms yet. 

We present in this paper an approach that could allow the detection and the
interpretation of gestures of a TV spectator from a simple and low cost camera. 
RevTV is the name of the French project for adding new interaction techniques
to TVs to allow the inclusion of the telespectator.

The final objective of this study is to control the animations of an avatar
that should be inserted within a TV show beside the presenter or actor.
One major scenario that we rely on, corresponds to some educational game where a pupil
controls his/her avatar which interact in a forecast program led by a ``real'' TV animator.
During such kind of scenarios, commands interaction
like pointing, selecting and moving, as well as natural gesture animation, are required.

% Il faut rajouter une description du projet Rev-TV
%Le dernier paragraphe n'est pas une description ?

\section{Gesture Semantics}
Symbolic gestures, such as emblems, play an important role in human
communication as they can fully take the place of words.
These gestures and spoken language are processed by the same system in the human brain.

Gestures do not come alone, they are combined with speech, or with cognitive
activities which can be detected from the interaction between two human in a
discussion, these type of gestures are used to add more precision on the details
and they are made by the same areas of the cortex \cite{SymbolicGest}.

% We study in this part the sources of gestures and the relation between
%them and other activities.

We try to extract some semantics of gestures by the analysis of the movements of
a user explaining a story and searching for a relation between gestures and the
part of the story being told \cite{gestureThought}.
%The result of this is the extraction of gesticulations . 

After extraction, most of gestures semantics can be divided into two parts for
their possible future use as:
\begin{description}
 \item[Animation Gestures] used to animate a virtual avatar.
 \item[Command Gestures] used to launch a specific predefined action.
\end{description}
%%%
% toute cette partie me semble incompréhensible... (entre la debut de la section 2 et la début de la 2.1)
% Je veux bien t'aider à la rédiger, mais je ne comprends pas vraiment ce que tu veux dire... 
% Quel est ton message ?

\ignore{
Gesture recognition got the attention of HCI researchers in order to find better
ways to talk to the computer. The first one was the Hand Gestural Cursor
(Put-That-There) aiming to the simplification of the means of communication
between human and the machine.
Since then, the gesture definition included more techniques and got applied on
the 2D recognition which is the multi-touch gestures.
}

\subsection{Gestures taxonomies}
A rapid look the possible gestures human can perform \cite{Gesturecraft},
lead to the conclusion that the amount of possibilities is tremendous.
Nonetheless, the gestures can be classified according to various criteria not
only to simplify their recognition, but also to allow their reproducing on an
avatar to putting tags on them for future analysis\cite{gestureRecreation}.
Here, we consider a taxonomy based on the context of the gestures as well as on their types:
%%%%%%%
%By looking at the possible gestures human can generate \cite{Gesturecraft}, we
%get a huge amount of possibilities which depend on the context and on the type.
%So, there is a need for a taxonomy.
%Many possible taxonomies exist according to the technical or analytical aspects:
%In order to describe and analyse each gesture, we can rely on the following aspects:
\ignore{\begin{description}
 \item[Gesture Creation Space:] We use this criteria to distinguish between gestures made in the 2D or 3D space.
 \item[Number of used strokes:] We use this criteria to distinguish between gestures constructed by only one path and between those using many paths, so the recognition needs to wait for all strokes before the recognition
 \item[Gesture context:] We use this criteria to distinguish gesture based on the specific use of it, gestures for disabled people, gestures in talk-shows, gestures in meetings.
\end{description}}
%Gesture and Thoughts, page 18
According to McNail \cite{gestureThought}, it is possible and useful to
distinguish between these gestures and a possible taxonomy can provide us these
types:
\begin{description}
 \item[Gesticulation] is a motion that embodies a meaning reliable to the
accompanying speech. It is made chiefly with the arms and hands but is not
restricted to these body parts.
 \item[Speech-linked gestures] are parts of sentences themselves, the gesture
completes the sentence structure.
 \item[Emblems] are conventionalized signs, such as thumbs-up or the ring (first
finger and thumb tips touching, other fingers extended) for ``OK.''
 \item[Pantomime] is dumb show, a gesture or sequence of gestures conveying a
narrative line, with a story to tell, produced without speech.
\end{description}

In our work, we focalize on emblems for commands and gesticulations, as both of
them are of prime importance for the scenario under consideration.
%We have chosen to analyze gesticulations as they are the most frequent.
%, and project them on the technical taxonomies.
%And we have also made our own extraction from Talk-Shows.
%Nous avons choisi celle-ci, car elle correspond aux besoins que nous avons
%identifié dans les scenarios d'utilisation de la télé interactive

\subsection{Gestures Library}

% Il faut rédiger complétement cette partie...
%C'est pas compréhensible, et il n'y a pas de verbe dans la seconde phrase.
In order to identify relevant gestures we have analyzed some talk-shows in order
to extract most frequent gestures being performed.
According to our scenario of use in the context of Interactive TV, we need
gesture to trigger commands but also for natural interaction with the avatar.
We have created a vector based gestures icons to represent them.
These 4 icons in the figure\ref{handg}, which are part of the ``ishara'' library,
represent the software supported gestures in our application.
%and defining a benchmark scenario.
%We have tried to target the following kinds of gestures in a first step.

\png{handg}{Family of supported gestures represented using the free gestures icons library from https://github.com/boulabiar/ishara }

Figure\ref{handg} shows our supported gestures. We provide a movement mode
used to select the 4 directions, then a multitouch, then a command mode, then a hand fingers recognition,
and, finally, a cursor movement mode.
%\pagebreak 
We have selected 4 basic gestures modes to be supported in our system as a first step.
Then to be extended depending on the context fulfilling the requirements
of our scenario.

\section{Technical Work}

In the figure \ref{pipeline}, we show the pipeline of the modules used to
extract gestural informations. Each part is described in details in the next
subsections.

\png{pipeline}{Pipeline of the work components showing the routing and
transformation of the camera information until getting the gestural informations
ready to use in the scenario}

\subsection{Used Materials}
We have chosen a low-cost camera for our work that can be integrated with other components in the future.
The PS3Eye camera matched our expectations with a 640x480 resolution combined with a 60 frames-per-second capability.
This camera costs about 40\$, which is a reasonable price for a potential market deployment.

\subsection{Color Transformations} %%%%%%%%%%%%%%%%%%%
The video processing is based on several modules, such as illustrated in Fig ... .
First, a basic but adaptive skin color segmentation of a large interval of human skin varieties is performed \cite{skinColorSeg}.
We also use a histogram equalisation technique to better enlarge the colors to all the space available.
Then we transform the color space from BGR, which come from the camera to YCrCb,
as it is the most adapted one to skin segmentation
Moreover, the corresponding transformation is linear \cite{skinColorSeg}, which helps computation time saving.

Other color spaces like HSV or CIE-Lab can be used for skin color segmentation,
but the transformation from BGR either takes more time or needs more parameters
in the next step.

\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Space} & BGR & HSV & YCrCb & CIE Lab\\
\hline
 & R\textgreater 95, G\textgreater 40, B\textgreater 20, & H[0.4 .. 0.7] & Cb[77 .. 127] & C[0 .. 65] \\
\textbf{Param.} & Max\{R,G,B\}-Min\{R,G,B\}\textless 15 & S[0.15 .. 0.75] & Cr[133 .. 173] & I[0 .. 14] \\
 & abs(R-G)\textgreater 15, R\textgreater G, R\textgreater B & V[0.35 .. 0.95] & & \\
\hline
\end{tabular}
\end{center}

\subsection{Input Filtering}
During this step, a 5x5 opening morphological kernel \cite{morphologicalAnalysis} is applied to smooth the results.
Practically, it cleans the image from isolated pixels and holes inside skin
zones.

\subsection{Blobs Detection}
Each blob is labeled in linear time using the contour tracing technique with
the linear-time component-labeling algorithm \cite{CompLabeling},
which requires 1 to 4 passes to recognize all blobs with their bounding contour.
We identify the hands and the head by a calibration focused on their first
position in the screen. We identify them and add a tag to their respective blobs.

\png{skincolor}{A representation of different human skin color variations}

\section{Blobs Tracking}
After having the hands and head blobs identified, an efficient
tracking method based on the appearance model for occlusion handling\cite{app06}.
We have used the cvblob implementation \cite{cvblob} for that algorithm which 
stores the measures of bounding boxes distance and provide tracks instead of just blobs.  %distantBlobTrack.

When the matrix of distance is set up, the coming loops
are executed to handle blobs changes:
\begin{itemize}
 \item A loop to detect inactive tracks: those tracks with no blobs near.
 \item Detect and create new tracks: those blobs without a track near.
 \item A loop to assign blobs to tracks. It makes clusters with blobs that are close and assign them to a track.
In this step some tracks could merge in one.
 \item A last loop which check all inactive tracks to delete the old ones.
\end{itemize}

To better handle blobs, the bounding path of the blobs
is simplified to a set of few points when needed.
This is used to identify picks for the counter mode.

\section{Gestural informations and Recognition modes}
The output of the video processing is the input of the recognition module,
which is based on the track location on the screen.
Four different modes are defined according to the hand locations. The switch between them is based on the following grammar:
\begin{description}
 \item[The movement mode:] it is activated when two hands are close to each other, so that the user selects between 4 directions. This mode can be used as input for games to select between the directions.
 \item[The multi-touch mode:] where we consider the similarities between 2D gestures in a tactile multi-touch context and gestures in the 3D space. To do so, we consider the hands blobs in a manner similar to that of two finger tips in the input of a multi-touch device. Based on that, we can recognize well known multi-touch gestures, such as Drag, Pinch/Zoom and Rotate with a consideration of a interaction centroid\cite{CentroidGest} but applied in our case.
 \item[The counter mode:] counts the number of fingers in a preselected hand (the left and right hand are automatically discriminated) by counting the peaks in the simplified contour of the blob. This mode can again be used in games for kids.
 \item[The mouse mode:] it is used to control a cursor to select something in an arbitrary place on the screen. With this mode, we only get the input from a sub zone in the screen and map it to the full screen. We use it in a similar way to a computer touchpad.
The user can validate clicks using a tempo on another zone.
\end{description}

A phase of calibration is triggered when the logic of tracking is lost or damaged.
We have used four modes in a first core with the possibility to add more sub-gestures in the future.
This core will be used to control a children game having to select, choose, and answer basic questions.
We have stick to that scenario to produce these gestures. 

%\pngl{point}{simplify}{List of modes}

\begin{figure}[!htb]
\centering
  \includegraphics[width=0.45\textwidth]{./pics/point.png}
  \includegraphics[width=0.4\textwidth]{./pics/simplify.png}
  \includegraphics[width=0.46\textwidth]{./pics/fingers.png}
  \includegraphics[width=0.46\textwidth]{./pics/move.png}
  \caption{A sub-list of recognition modes showing pointing, number of hand
fingers counting and the movement mode}
%  \label{fig:Gestes}
\end{figure}


\section{Usability and Naturalness in an ubiquitous environment}

The usability of our system should be compared to other systems in an ubiquitous
environment. In our case, we have a TV instead of a PC.
The gestures in such environment, and specially for children game scenario,
are made for a short period of time, and doesn't require a good usability,
even if we can recognize gestures in real-time.

The Naturalness of gestures is supported by the analysis of those to be
supported and the preference of gesticulations.
Gesticulations are the most common gestures, so they inherit from this their
ability to be produced with less complication.

Emblems gestures which englobe commands, are chosen to be produced in more
difficult positions to allow easy discrimination from gesticulations.
Even with these extractions, the naturalness aspect of the gestural interaction
itself are still objected.  \cite{naturalNorman2010}

Spacial gestural interactions always lack from the feedback for the interaction.
This affects also the usability because the user can no more be guided in space
as he was with surface movement or simple physical joysticks.

\section{System integration of this interaction and future work}
Human brain perisylvian areas, identified as the core of the brain's language
system, are not in fact committed to language processing, but may function as a
modality-independent semiotic system that plays a broader role in human
communication, linking meaning with symbols whether these are words, gestures,
images, sounds, or objects \cite{SymbolicGest}.
This natural handling of multimodal communications gives an argument to build
recognizer taking part of most of these modalities specially to remove ambiguous
gesture meaning decision cases.

Our system only supports gestural interaction at the moment but the integration
of other multimodal means are possible to fine tune the input and give the user
better feedback.
These possible evolution of the system are possible:

\begin{description}
 \item[Facial Recognition]: our system don't take care of the facial expressions
recogntions. A future support in this area can be added for more reactiveness in
pupil's games.
 \item[Voice Recognition]: To speed up commands handling, we can use short voice
keywords either to move from one mode to another, or to select object.
 \item[Haptic feedback]: A special wearable vest can be used to allow more
reactiveness with the user. But this area still lacking innovation because the
haptic feedback is by area and not continue.
\end{description}

\section*{Conclusion}
In this paper we have shown a proof-of-concept mechanism to recognize hand
gestures then use them to control a TV or to take actions in a Game scenario.
We have also freely provided a library which can be used to represent gestures
in other scenarios.
The evolution of our system is being discussed to add more modalities even if we
are aware of some of the limits \cite{Oviatt99tenmyths}.

% ---- Bibliography ----
\bibliographystyle{splncs03}
\bibliography{paper}

\end{document}
